{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ffc012-5d64-4b5c-9f9f-ea56e026ab4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read all CSVs into Spark DataFrames\n",
    "df_customers = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_customers_dataset.csv\")\n",
    "df_geolocation = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_geolocation_dataset.csv\")\n",
    "df_order_items = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_order_items_dataset.csv\")\n",
    "df_order_payments = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_order_payments_dataset.csv\")\n",
    "df_order_reviews = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_order_reviews_dataset.csv\")\n",
    "df_orders = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_orders_dataset.csv\")\n",
    "df_products = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_products_dataset.csv\")\n",
    "df_sellers = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_sellers_dataset.csv\")\n",
    "df_product_category_name_translation = spark.read.option(\"header\", True).csv(\"dbfs:/Volumes/commerce_spark_workspace/default/ecommerce_raw/product_category_name_translation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a090cc-f19c-4e53-8233-b572e8cb9873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders = df_orders.select(\"order_id\", \"customer_id\", \"order_status\", \"order_purchase_timestamp\")\n",
    "df_order_items = df_order_items.select(\"order_id\", \"order_item_id\", \"product_id\", \"seller_id\", \"price\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b930593a-4850-435f-90ee-b7f031c84ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers = spark.read.option(\"header\", True).csv(\"file:/dbfs/Volumes/commerce_spark_workspace/default/ecommerce_raw/olist_customers_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca85ab-407e-4c6e-919a-228ebd7991b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+------------------------+----------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+-------------+-------------------+------+-------------+----------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+----------------------+--------------+------------+--------------------------------+------------+--------------------+---------------------------------------------------------------------------+--------------------+-----------------------+\n|order_id                        |seller_id                       |product_id                      |customer_id                     |customer_unique_id              |customer_zip_code_prefix|customer_city   |customer_state|order_status|order_purchase_timestamp|order_approved_at  |order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|order_item_id|shipping_limit_date|price |freight_value|product_category_name |product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|seller_zip_code_prefix|seller_city   |seller_state|review_id                       |review_score|review_comment_title|review_comment_message                                                     |review_creation_date|review_answer_timestamp|\n+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+------------------------+----------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+-------------+-------------------+------+-------------+----------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+----------------------+--------------+------------+--------------------------------+------------+--------------------+---------------------------------------------------------------------------+--------------------+-----------------------+\n|ccbabeb0b02433bd0fcbac46e70339f2|16090f2ca825584b5a147ab24aa30c86|89321f94e35fc6d7903d36f74e351d40|c77ee2d8ba1614a4d489a44166894938|9c9cef121cb812cb301babddc2d8331e|38067                   |uberaba         |MG            |delivered   |2018-02-19 20:31:09     |2018-02-21 06:15:25|2018-02-22 21:04:23         |2018-03-09 22:22:25          |2018-03-13 00:00:00          |1            |2018-02-27 03:31:34|27.90 |15.10        |alimentos             |59                 |982                       |1                 |150             |17               |13               |13              |12940                 |atibaia       |SP          |8972e42b54449efa29204423f1a9ead7|4           |NULL                |NULL                                                                       |2018-03-10 00:00:00 |2018-03-11 21:08:38    |\n|c6bf92017bd40729c135b58b643f64c2|36a968b544695394e4e9d7572688598f|3f1a741cf5591384428c1cbb0ef07ec0|3d3c463710ea6e8dd9a63c1110eeb06b|55c00c8a161d2e6d7d731dd87341ad2f|13175                   |sumare          |SP            |delivered   |2018-08-08 01:15:06     |2018-08-09 07:44:53|2018-08-10 14:43:00         |2018-08-15 00:18:43          |2018-08-16 00:00:00          |1            |2018-08-13 07:44:53|14.90 |7.39         |beleza_saude          |60                 |400                       |1                 |150             |60               |11               |16              |11010                 |santos        |SP          |bd255976fef34fdc72428a574f9810f7|5           |Ótimo               |Perfeito e recomendo chegou dentro do prazo                                |2018-08-16 00:00:00 |2018-08-16 23:12:44    |\n|ab87dc5a5f1856a10640d5f42e4c2fd9|b410bdd36d5db7a65dcd42b7ead933b8|f8b624d4e475bb8d1bddf1b65c6a64f6|538a4d02876412846b966a3c057395e5|fd317ae6d8988b7041034edac3f253bf|38040                   |uberaba         |MG            |delivered   |2018-06-04 12:38:45     |2018-06-04 12:50:47|2018-06-05 12:12:00         |2018-06-11 20:39:49          |2018-07-17 00:00:00          |1            |2018-06-12 12:50:47|179.00|31.90        |utilidades_domesticas |46                 |323                       |1                 |2841            |40               |23               |26              |74645                 |goiania       |GO          |0e58a53b2fd4c5c97cbfcda639055e66|5           |NULL                |NULL                                                                       |2018-06-12 00:00:00 |2018-06-13 01:09:08    |\n|06ff862a85c2402aa52dc9edf150bf30|7e3f87d16fb353f408d467e74fbd8014|4ce9ab528124f89e091b17d11aa2e97c|0a978c825ff7d013133ddc7f77566172|dc9431e47beadfe1188b67bec3717969|60710                   |fortaleza       |CE            |delivered   |2017-11-30 13:31:08     |2017-12-01 11:30:56|2017-12-01 17:51:53         |2017-12-28 22:27:57          |2017-12-28 00:00:00          |1            |2017-12-07 04:49:47|41.90 |17.63        |informatica_acessorios|54                 |222                       |3                 |500             |36               |5                |15              |04809                 |sao paulo     |SP          |016623a7df4a6496b81603b084d4d941|3           |NULL                |NULL                                                                       |2017-12-29 00:00:00 |2018-01-01 14:48:28    |\n|f23155f5fa9b826631c5b8e038b38393|74c7dec0a384d8a05950e629bd23bde9|052b8660ee8a9ee18815d9b276694a10|21a99191298d34fb6dd0b088e821591c|398fbb883435515de0aefe3887d6ea10|22610                   |rio de janeiro  |RJ            |delivered   |2017-09-20 12:19:12     |2017-09-21 02:45:30|2017-09-26 19:22:57         |2017-10-02 21:43:57          |2017-10-13 00:00:00          |1            |2017-09-27 02:45:30|19.99 |15.10        |perfumaria            |47                 |540                       |2                 |107             |17               |12               |14              |13845                 |mogi guacu    |SP          |ac107b8f1b0a7a7fb2e87edb756e6335|5           |NULL                |NULL                                                                       |2017-10-03 00:00:00 |2017-10-04 02:23:29    |\n|69fd81b0cd556f5da5000c1ed874ed19|080102cd0a76b09e0dcf55fcacc60e05|28f61ad35fb219e9debd750a73b63985|6ad71323c11ba8a83737ccc3ea31fbc3|c739d7adb6f103eae180b1f2fc74455f|89284                   |sao bento do sul|SC            |delivered   |2017-09-25 10:53:40     |2017-09-25 11:05:35|2017-09-25 18:06:56         |2017-09-28 18:53:14          |2017-10-19 00:00:00          |1            |2017-09-29 11:05:35|42.79 |16.79        |informatica_acessorios|55                 |191                       |2                 |100             |40               |2                |24              |31140                 |belo horizonte|MG          |8af8b6f27fd4468c4809b1d782c30a28|5           |NULL                |Produto chegou em excelente estado e antes do prazo, obrigado pela atenção |2017-09-29 00:00:00 |2017-09-29 23:38:52    |\n|d40dd8018a5302969efb31bd21744cab|8e6cc767478edae941d9bd9eb778d77a|2284b28ca179d66957a67ef01a5b7d6c|0470c47f1dd7a91d0f3b8a420589e0f7|27c5a2e7ca50a650565c75052cd5712d|35700                   |sete lagoas     |MG            |delivered   |2017-03-23 22:59:21     |2017-03-23 23:10:30|2017-03-25 09:26:22         |2017-04-06 16:33:45          |2017-04-13 00:00:00          |1            |2017-04-02 23:10:30|35.00 |14.12        |utilidades_domesticas |54                 |408                       |1                 |1600            |34               |21               |23              |38442                 |araguari      |MG          |167a2bbd26a3f7b8de628ce5a0329ab0|3           |NULL                |NULL                                                                       |2017-04-07 00:00:00 |2017-04-08 11:27:31    |\n|42560dfc8d7863a190293678f01f6bbd|f181738b150df1f37cb0bd72e705b193|f5d9f6be389c406755cbe9f20954dd9a|0f3a81be69f12da7e2979fd1833e923d|bb939798f4c3086488aba189af45cc1c|05427                   |sao paulo       |SP            |delivered   |2017-10-22 01:39:47     |2017-10-22 01:49:12|2017-10-23 14:07:54         |2017-10-24 17:09:24          |2017-11-06 00:00:00          |2            |2017-10-27 01:49:12|14.90 |7.78         |utilidades_domesticas |40                 |391                       |1                 |200             |37               |3                |11              |06317                 |carapicuiba   |SP          |05cd167a955ccc0a9080ec1ad632c985|5           |NULL                |NULL                                                                       |2017-10-25 00:00:00 |2017-10-26 17:13:44    |\n|3f003568147c785083d014edfba38c48|08d0949a9a17c027262db1f3c450c26c|8d5ab785e6761f35bf54eca83846dd2e|72d90899884781ae2fc19e49cc102fc0|35da8188d2ec392ebd41e4d29d68c14a|88052                   |florianopolis   |SC            |delivered   |2018-06-18 16:58:12     |2018-06-18 17:20:37|2018-06-26 15:27:00         |2018-07-02 22:49:09          |2018-07-17 00:00:00          |1            |2018-06-22 17:20:37|12.90 |18.23        |moveis_decoracao      |57                 |401                       |1                 |350             |32               |2                |22              |85804                 |cascavel      |PR          |54fb8457b4672719adb47e41e6d00e08|1           |1                   |MUITO MAU ACABADO NÃO RECOMENDO                                            |2018-07-03 00:00:00 |2018-07-04 05:54:35    |\n|db192ddb0ea5a4d7a0506e7d7ec92343|7142540dd4c91e2237acb7e911c4eba2|5dc080b3e2c10d388f9a73fa2a49cfb3|c21e33cbee939409efff8c2ef959a275|bcf6059e426fbda8269e24b0cbade2d2|15850                   |urupes          |SP            |delivered   |2018-04-17 17:03:28     |2018-04-17 17:15:17|2018-04-18 21:50:42         |2018-04-23 20:11:24          |2018-05-07 00:00:00          |1            |2018-04-23 17:15:17|59.90 |14.58        |automotivo            |59                 |703                       |6                 |5850            |47               |19               |23              |16301                 |penapolis     |SP          |aca64e3ebc36369c427cc5741fddcd6f|5           |NULL                |Tapete muito bom... e da pra personalizar o tamanho cortando               |2018-04-24 00:00:00 |2018-04-26 23:43:19    |\n+--------------------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+------------------------+----------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+-------------+-------------------+------+-------------+----------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+----------------------+--------------+------------+--------------------------------+------------+--------------------+---------------------------------------------------------------------------+--------------------+-----------------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Join customers → orders → order_items → products → sellers → reviews\n",
    "df_all = (df_customers\n",
    "    .join(df_orders, \"customer_id\", \"inner\")\n",
    "    .join(df_order_items, \"order_id\", \"inner\")\n",
    "    .join(df_products, \"product_id\", \"inner\")\n",
    "    .join(df_sellers, \"seller_id\", \"inner\")\n",
    "    .join(df_order_reviews, \"order_id\", \"left\")  # left join for reviews (optional)\n",
    ")\n",
    "\n",
    "df_all.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5424f1-d13f-4b32-a26a-3bab63456975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimized Joins For Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecf46ba-5096-426b-8698-008cee02e21b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------+\n|customer_id                     |total_orders|\n+--------------------------------+------------+\n|0bd683b7ceca26b5bba4e327682275c5|1           |\n|ba76714c4894372325ea2d044823344a|1           |\n|f7398fc942c8fa80e5419ae52e49f7fb|1           |\n|66e8039d5fddd75067de6b08e4bb22e7|1           |\n|04fc2ecbb192c71163629f423d57a13d|1           |\n|d0b0b2dd8bdaf36ebea19c232b4e986b|1           |\n|69009aaf4e400602c05b1da3989ca40b|1           |\n|b1e99a86b163f1f25e7e0fa3360ad93d|1           |\n|f3457b8fdac18622de498551383ae1cc|1           |\n|c2928a50aecf1bc4776082b13225e4da|1           |\n+--------------------------------+------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Join customers with orders\n",
    "df_orders_per_customer = df_customers.join(df_orders, \"customer_id\", \"inner\") \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(count(\"order_id\").alias(\"total_orders\")) \\\n",
    "    .orderBy(\"total_orders\", ascending=False)\n",
    "\n",
    "df_orders_per_customer.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2241adf9-9193-4928-bc2e-043c3ba0d094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------+\n|seller_id                       |avg_review_score|\n+--------------------------------+----------------+\n|e1c76f339ebd5460999f25a2aa8c92c5|5.0             |\n|8c9348f33ae3dada25c99c99ade2af78|5.0             |\n|05730013efda596306417c3b09302475|5.0             |\n|702835e4b785b67a084280efca355756|5.0             |\n|4a82c4af97ffc0fb2dc26bfdc03b1842|5.0             |\n|1e47defeeadeca0e9a18fa5a9311e735|5.0             |\n|fd312b6bf05efac6c3772d5b52205d8a|5.0             |\n|44717f64ec2a457979cf83c429077666|5.0             |\n|a64e44665225d19dfc0277eeeaaccc57|5.0             |\n|2fa13c8bd5705d279f7ed5cc9ec61c68|5.0             |\n+--------------------------------+----------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Join sellers with order items, orders, and reviews\n",
    "df_avg_review_per_seller = df_sellers \\\n",
    "    .join(df_order_items, \"seller_id\", \"inner\") \\\n",
    "    .join(df_orders, \"order_id\", \"inner\") \\\n",
    "    .join(df_order_reviews, \"order_id\", \"inner\") \\\n",
    "    .groupBy(\"seller_id\") \\\n",
    "    .agg(avg(\"review_score\").alias(\"avg_review_score\")) \\\n",
    "    .orderBy(\"avg_review_score\", ascending=False)\n",
    "\n",
    "df_avg_review_per_seller.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175c887d-22ea-44b1-a7af-dbe2666a8be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n|          product_id|total_sold|\n+--------------------+----------+\n|aca2eb7d00ea1a7b8...|       527|\n|99a4788cb24856965...|       488|\n|422879e10f4668299...|       484|\n|389d119b48cf3043d...|       392|\n|368c6c730842d7801...|       388|\n|53759a2ecddad2bb8...|       373|\n|d1c427060a0f73f6b...|       343|\n|53b36df67ebb7c415...|       323|\n|154e7e31ebfa09220...|       281|\n|3dd2a17168ec895c7...|       274|\n+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_top_products = df_order_items \\\n",
    "    .groupBy(\"product_id\") \\\n",
    "    .agg(count(\"*\").alias(\"total_sold\")) \\\n",
    "    .orderBy(\"total_sold\", ascending=False) \\\n",
    "    .limit(10)\n",
    "\n",
    "df_top_products.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a6de22-39f5-488b-89cf-a464e428cddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n|         customer_id|      total_spent|\n+--------------------+-----------------+\n|1617b1357756262bf...|  13664.080078125|\n|ec5b2ba62e5743423...|  7274.8798828125|\n|c6e2731c5b391845f...| 6929.31005859375|\n|f48d464a0baaea338...|  6922.2099609375|\n|3fd6777bbce08a352...|    6726.66015625|\n|05455dfa7cd02f13d...|6081.539794921875|\n|df55c14d1476a9a34...|    4950.33984375|\n|e0a2412720e9ea4f2...| 4809.43994140625|\n|24bbf5fd2f2e1b359...|    4764.33984375|\n|3d979689f636322c6...| 4681.77978515625|\n+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "df_top_customers = df_order_items \\\n",
    "    .join(df_orders, \"order_id\", \"inner\") \\\n",
    "    .join(df_customers, \"customer_id\", \"inner\") \\\n",
    "    .withColumn(\"total_price\", col(\"price\").cast(\"float\") + col(\"freight_value\").cast(\"float\")) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(_sum(\"total_price\").alias(\"total_spent\")) \\\n",
    "    .orderBy(col(\"total_spent\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "df_top_customers.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c761c7d0-3096-40c9-99f3-a4a82e6c71cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Window Functions And Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ac8574-b962-48e0-97e5-91a2e84030c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----+\n|           seller_id|          product_id|total_sold|rank|\n+--------------------+--------------------+----------+----+\n|0015a82c2db000af6...|a2ff5a97bf95719e3...|       3.0|   1|\n|001cca7ae9ae17fb1...|08574b074924071f4...|     131.0|   1|\n|001cca7ae9ae17fb1...|e251ebd2858be1aa7...|      67.0|   2|\n|001cca7ae9ae17fb1...|98a8c2fa16d7239c6...|      33.0|   3|\n|001cca7ae9ae17fb1...|547b95702aec86f05...|      23.0|   4|\n|001cca7ae9ae17fb1...|0da9ffd92214425d8...|      18.0|   5|\n|001cca7ae9ae17fb1...|86b22a03cb72239dd...|      12.0|   6|\n|001cca7ae9ae17fb1...|21fecd254a3103704...|       3.0|   7|\n|001cca7ae9ae17fb1...|4d7fee7877228c149...|       2.0|   8|\n|001cca7ae9ae17fb1...|4f3b83b83f7fb280f...|       1.0|   9|\n|001cca7ae9ae17fb1...|6d15a14a5c04e3ef3...|       1.0|   9|\n|001cca7ae9ae17fb1...|d32f22e03fb01595a...|       1.0|   9|\n|001e6ad469a905060...|093cd981b714bcdff...|       1.0|   1|\n|002100f778ceb8431...|158102fe543dbaeb8...|      18.0|   1|\n|002100f778ceb8431...|414c53fa9c5cc4a03...|      14.0|   2|\n|002100f778ceb8431...|2ceff1056fe5bd4ee...|       3.0|   3|\n|002100f778ceb8431...|9666f6aaf24141e6d...|       2.0|   4|\n|002100f778ceb8431...|79792fabf714677e9...|       2.0|   4|\n|002100f778ceb8431...|cb790fa02cc6097dc...|       2.0|   4|\n|002100f778ceb8431...|1b7c03297b91c3acc...|       2.0|   4|\n+--------------------+--------------------+----------+----+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Top selling products per seller\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as _sum, rank\n",
    "\n",
    "# Aggregate total sold per product per seller\n",
    "df_seller_product_sales = df_order_items \\\n",
    "    .groupBy(\"seller_id\", \"product_id\") \\\n",
    "    .agg(_sum(\"order_item_id\").alias(\"total_sold\"))\n",
    "\n",
    "# Define window partitioned by seller\n",
    "window_spec = Window.partitionBy(\"seller_id\").orderBy(col(\"total_sold\").desc())\n",
    "\n",
    "# Rank products within each seller\n",
    "df_ranked_products = df_seller_product_sales \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "df_ranked_products.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a25063a1-4913-4916-a298-1af34882d8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced Aggregation and Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd47c2a8-6f4e-4c40-aa6a-7cb39046d51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+------------+------+\n|customer_id                     |total_revenue     |total_orders|AOV   |\n+--------------------------------+------------------+------------+------+\n|f3457b8fdac18622de498551383ae1cc|166.16            |1           |166.16|\n|2306cc5d9784d518dce0f1d7649b1d61|276.35            |1           |276.35|\n|809c3dc9eabbbe4e6b03198d239b6e2d|205.87            |1           |205.87|\n|f95a88c152cc3320d2e6ab9efb9cc90c|424.03            |1           |424.03|\n|05860a17b17441a53e85b2c0e2daf737|156.16            |1           |156.16|\n|c55751576fef472fbe91aa1fe165a581|69.33             |1           |69.33 |\n|ee039bb5e02a2f5eab3da733e88dbec3|140.32            |1           |140.32|\n|a2141dae4f56fd98148f8d8956f2c10a|70.03             |1           |70.03 |\n|fdd0876705d2f479feeb936cd9cad782|721.74            |1           |721.74|\n|43d9a8df81f815755d9a258f2ee3135f|144.54            |1           |144.54|\n|d4d2ea890454df93e3ce1c29627135b2|72.37             |1           |72.37 |\n|7a24a27590128a1f2925250e63fc9670|35.68             |1           |35.68 |\n|b168b6c101fbb2ba0b62622259b9e2e1|135.07999999999998|1           |135.08|\n|69009aaf4e400602c05b1da3989ca40b|96.8              |1           |96.8  |\n|84022f0b4a55c6fa83660c019d91ec08|32.33             |1           |32.33 |\n|56283bef437a7c418696a496c73b9a89|158.07            |1           |158.07|\n|2d4e9c0e42af45f4b002376d45f7d46c|27.4              |1           |27.4  |\n|f7a143ca85eaadf2b2f56394e2d1f699|75.84             |1           |75.84 |\n|368dbdc31e230a411c5d91546ab60ea3|174.36            |1           |174.36|\n|8802f2a5879f0b53708e5ee21358792a|196.22            |1           |196.22|\n+--------------------------------+------------------+------------+------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#calculate total revenue and average order value (AOV) per customer\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, sum as _sum, countDistinct, round\n",
    "\n",
    "# Cast string columns to Double\n",
    "df_order_items_casted = df_order_items \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(DoubleType())) \\\n",
    "    .withColumn(\"freight_value\", col(\"freight_value\").cast(DoubleType()))\n",
    "\n",
    "# Join with orders table to get customer_id\n",
    "df_customer_orders = df_order_items_casted \\\n",
    "    .join(df_orders, \"order_id\", \"inner\")\n",
    "\n",
    "# Calculate revenue and aggregation\n",
    "df_customer_revenue = df_customer_orders \\\n",
    "    .withColumn(\"order_revenue\", col(\"price\") + col(\"freight_value\")) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        _sum(\"order_revenue\").alias(\"total_revenue\"),\n",
    "        countDistinct(\"order_id\").alias(\"total_orders\")\n",
    "    )\n",
    "\n",
    "# Calculate AOV\n",
    "df_customer_revenue = df_customer_revenue \\\n",
    "    .withColumn(\"AOV\", round(col(\"total_revenue\") / col(\"total_orders\"), 2))\n",
    "\n",
    "df_customer_revenue.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af32577-1d8c-4c8f-b476-4216c71c5368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, countDistinct, col\n",
    "\n",
    "# Convert columns to appropriate numeric types since CSV loads as strings by default\n",
    "df_order_items = df_order_items.withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "                               .withColumn(\"freight_value\", col(\"freight_value\").cast(\"double\")) \\\n",
    "                               .withColumn(\"order_id\", col(\"order_id\").cast(\"string\"))\n",
    "\n",
    "df_order_reviews = df_order_reviews.withColumn(\"review_score\", col(\"review_score\").cast(\"double\")) \\\n",
    "                                   .withColumn(\"order_id\", col(\"order_id\").cast(\"string\"))\n",
    "\n",
    "df_orders = df_orders.withColumn(\"order_id\", col(\"order_id\").cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecd23ef-8032-4e5d-a7e5-73263e8adf0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out rows where numeric columns are null (optional but recommended)\n",
    "df_order_items_filtered = df_order_items.filter(col(\"price\").isNotNull() & col(\"freight_value\").isNotNull())\n",
    "df_order_reviews_filtered = df_order_reviews.filter(col(\"review_score\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55d98ea-afe8-43ee-9166-27406616d947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total revenue from order_items\n",
    "total_revenue = df_order_items.select(\n",
    "    (sum(col(\"price\") + col(\"freight_value\"))).alias(\"total_revenue\")\n",
    ").collect()[0][\"total_revenue\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4f97f7-0ee4-4576-ab77-e24a8c6c24b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total orders from orders dataframe\n",
    "total_orders = df_orders.select(\n",
    "    countDistinct(col(\"order_id\")).alias(\"total_orders\")\n",
    ").collect()[0][\"total_orders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab686893-1640-4bb1-9e3f-2d44b6ae74f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenue: 15843553.240000086\nTotal Orders: 99441\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Revenue: {total_revenue}\")\n",
    "\n",
    "print(f\"Total Orders: {total_orders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176795fc-a2fc-4f52-8c88-f3efa72cf966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5926941429959711>, line 21\u001B[0m\n",
       "\u001B[1;32m     14\u001B[0m df_reviews_with_product \u001B[38;5;241m=\u001B[39m df_order_reviews\u001B[38;5;241m.\u001B[39mjoin(\n",
       "\u001B[1;32m     15\u001B[0m     df_order_items\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdistinct(),\n",
       "\u001B[1;32m     16\u001B[0m     on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     17\u001B[0m     how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m )\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Check schema and columns after join - just to confirm\u001B[39;00m\n",
       "\u001B[0;32m---> 21\u001B[0m df_reviews_with_product\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\u001B[1;32m     22\u001B[0m df_reviews_with_product\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# 4. Calculate average review score per product\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2027\u001B[0m, in \u001B[0;36mDataFrame.printSchema\u001B[0;34m(self, level)\u001B[0m\n",
       "\u001B[1;32m   2025\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mtreeString(level))\n",
       "\u001B[1;32m   2026\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 2027\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mtreeString())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2000\u001B[0m, in \u001B[0;36mDataFrame.schema\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1998\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1999\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2000\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mschema(query)\n",
       "\u001B[1;32m   2001\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1268\u001B[0m, in \u001B[0;36mSparkConnectClient.schema\u001B[0;34m(self, plan)\u001B[0m\n",
       "\u001B[1;32m   1266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m logger\u001B[38;5;241m.\u001B[39misEnabledFor(logging\u001B[38;5;241m.\u001B[39mINFO):\n",
       "\u001B[1;32m   1267\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSchema for plan: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_proto_to_string(plan,\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m-> 1268\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mplan)\u001B[38;5;241m.\u001B[39mschema\n",
       "\u001B[1;32m   1269\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1270\u001B[0m \u001B[38;5;66;03m# Server side should populate the struct field which is the schema.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1546\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1545\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1546\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2133\u001B[0m                 info,\n",
       "\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`order_id`, `review_id`, `review_score`, `review_comment_title`, `review_creation_date`]. SQLSTATE: 42703;\n",
       "'Join UsingJoin(LeftOuter, [order_id])\n",
       ":- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n",
       ":  +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n",
       ":     +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n",
       ":        +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n",
       ":           +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n",
       ":              +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n",
       ":                 +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n",
       ":                    +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n",
       ":                       +- 'Project [review_id#13751, order_id#14983, review_score#14988, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757, cast('product_id as string) AS product_id#14989]\n",
       ":                          +- Project [review_id#13751, order_id#14983, try_cast(review_score#14987 as double) AS review_score#14988, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n",
       ":                             +- Project [review_id#13751, order_id#14983, try_cast(review_score#14986 as double) AS review_score#14987, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n",
       ":                                +- Project [review_id#13751, order_id#14983, cast(review_score#14985 as double) AS review_score#14986, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n",
       ":                                   +- Project [review_id#13751, order_id#14983, try_cast(review_score#14984 as double) AS review_score#14985, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n",
       ":                                      +- Project [review_id#13751, order_id#14983, cast(review_score#14982 as double) AS review_score#14984, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n",
       ":                                         +- Project [review_id#13751, cast(order_id#13752 as string) AS order_id#14983, review_score#14982, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n",
       ":                                            +- Project [review_id#13751, order_id#13752, cast(review_score#13753 as double) AS review_score#14982, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n",
       ":                                               +- Relation [review_id#13751,order_id#13752,review_score#13753,review_comment_title#13754,review_comment_message#13755,review_creation_date#13756,review_answer_timestamp#13757] csv\n",
       "+- Deduplicate [order_id#14358, product_id#14357]\n",
       "   +- Project [order_id#14358, product_id#14357]\n",
       "      +- Project [order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14359, try_cast(freight_value#14356 as double) AS freight_value#14360, product_quantity#14343, quantity#14344]\n",
       "         +- Project [order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, try_cast(price#14355 as double) AS price#14359, freight_value#14356, product_quantity#14343, quantity#14344]\n",
       "            +- Project [cast(order_id#14354 as string) AS order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14355, freight_value#14356, product_quantity#14343, quantity#14344]\n",
       "               +- Project [order_id#14354, order_item_id#14342, cast(product_id#14353 as string) AS product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14355, freight_value#14356, product_quantity#14343, quantity#14344]\n",
       "                  +- Project [order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14355, try_cast(freight_value#14352 as double) AS freight_value#14356, product_quantity#14343, quantity#14344]\n",
       "                     +- Project [order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, try_cast(price#14351 as double) AS price#14355, freight_value#14352, product_quantity#14343, quantity#14344]\n",
       "                        +- Project [cast(order_id#14350 as string) AS order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14351, freight_value#14352, product_quantity#14343, quantity#14344]\n",
       "                           +- Project [order_id#14350, order_item_id#14342, cast(product_id#14349 as string) AS product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14351, freight_value#14352, product_quantity#14343, quantity#14344]\n",
       "                              +- Project [order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14351, try_cast(freight_value#14348 as double) AS freight_value#14352, product_quantity#14343, quantity#14344]\n",
       "                                 +- Project [order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, try_cast(price#14347 as double) AS price#14351, freight_value#14348, product_quantity#14343, quantity#14344]\n",
       "                                    +- Project [cast(order_id#14346 as string) AS order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14347, freight_value#14348, product_quantity#14343, quantity#14344]\n",
       "                                       +- Project [order_id#14346, order_item_id#14342, cast(product_id#14345 as string) AS product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14347, freight_value#14348, product_quantity#14343, quantity#14344]\n",
       "                                          +- Project [order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14347, try_cast(freight_value#14341 as double) AS freight_value#14348, product_quantity#14343, quantity#14344]\n",
       "                                             +- Project [order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, try_cast(price#14340 as double) AS price#14347, freight_value#14341, product_quantity#14343, quantity#14344]\n",
       "                                                +- Project [cast(order_id#14339 as string) AS order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, quantity#14344]\n",
       "                                                   +- Project [order_id#14339, order_item_id#14342, cast(product_id#14338 as string) AS product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, quantity#14344]\n",
       "                                                      +- Project [order_id#14339, order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, 1 AS quantity#14344]\n",
       "                                                         +- Project [order_id#14339, order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, try_cast(order_item_id#14342 as int) AS product_quantity#14343]\n",
       "                                                            +- Project [order_id#14339, cast(order_item_id#13278 as string) AS order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341]\n",
       "                                                               +- Project [order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, try_cast(freight_value#14336 as double) AS freight_value#14341]\n",
       "                                                                  +- Project [order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, try_cast(price#14335 as double) AS price#14340, freight_value#14336]\n",
       "                                                                     +- Project [cast(order_id#14337 as string) AS order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n",
       "                                                                        +- Project [order_id#14337, order_item_id#13278, cast(product_id#13279 as string) AS product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n",
       "                                                                           +- Project [cast(order_id#13277 as string) AS order_id#14337, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n",
       "                                                                              +- Project [order_id#13277, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, price#14335, cast(freight_value#13283 as double) AS freight_value#14336]\n",
       "                                                                                 +- Project [order_id#13277, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, cast(price#13282 as double) AS price#14335, freight_value#13283]\n",
       "                                                                                    +- Relation [order_id#13277,order_item_id#13278,product_id#13279,seller_id#13280,shipping_limit_date#13281,price#13282,freight_value#13283] csv\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:552)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
       "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
       "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:427)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:427)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n",
       "\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:401)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:133)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:144)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:107)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:69)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:69)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:54)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:260)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:53)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:114)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n",
       "\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:382)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:345)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:367)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:345)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:563)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:343)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:375)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:336)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:382)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`order_id`, `review_id`, `review_score`, `review_comment_title`, `review_creation_date`]. SQLSTATE: 42703;\n'Join UsingJoin(LeftOuter, [order_id])\n:- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:  +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:     +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:        +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:           +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:              +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:                 +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:                    +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:                       +- 'Project [review_id#13751, order_id#14983, review_score#14988, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757, cast('product_id as string) AS product_id#14989]\n:                          +- Project [review_id#13751, order_id#14983, try_cast(review_score#14987 as double) AS review_score#14988, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                             +- Project [review_id#13751, order_id#14983, try_cast(review_score#14986 as double) AS review_score#14987, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                +- Project [review_id#13751, order_id#14983, cast(review_score#14985 as double) AS review_score#14986, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                   +- Project [review_id#13751, order_id#14983, try_cast(review_score#14984 as double) AS review_score#14985, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                      +- Project [review_id#13751, order_id#14983, cast(review_score#14982 as double) AS review_score#14984, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                         +- Project [review_id#13751, cast(order_id#13752 as string) AS order_id#14983, review_score#14982, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                            +- Project [review_id#13751, order_id#13752, cast(review_score#13753 as double) AS review_score#14982, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                               +- Relation [review_id#13751,order_id#13752,review_score#13753,review_comment_title#13754,review_comment_message#13755,review_creation_date#13756,review_answer_timestamp#13757] csv\n+- Deduplicate [order_id#14358, product_id#14357]\n   +- Project [order_id#14358, product_id#14357]\n      +- Project [order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14359, try_cast(freight_value#14356 as double) AS freight_value#14360, product_quantity#14343, quantity#14344]\n         +- Project [order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, try_cast(price#14355 as double) AS price#14359, freight_value#14356, product_quantity#14343, quantity#14344]\n            +- Project [cast(order_id#14354 as string) AS order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14355, freight_value#14356, product_quantity#14343, quantity#14344]\n               +- Project [order_id#14354, order_item_id#14342, cast(product_id#14353 as string) AS product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14355, freight_value#14356, product_quantity#14343, quantity#14344]\n                  +- Project [order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14355, try_cast(freight_value#14352 as double) AS freight_value#14356, product_quantity#14343, quantity#14344]\n                     +- Project [order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, try_cast(price#14351 as double) AS price#14355, freight_value#14352, product_quantity#14343, quantity#14344]\n                        +- Project [cast(order_id#14350 as string) AS order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14351, freight_value#14352, product_quantity#14343, quantity#14344]\n                           +- Project [order_id#14350, order_item_id#14342, cast(product_id#14349 as string) AS product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14351, freight_value#14352, product_quantity#14343, quantity#14344]\n                              +- Project [order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14351, try_cast(freight_value#14348 as double) AS freight_value#14352, product_quantity#14343, quantity#14344]\n                                 +- Project [order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, try_cast(price#14347 as double) AS price#14351, freight_value#14348, product_quantity#14343, quantity#14344]\n                                    +- Project [cast(order_id#14346 as string) AS order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14347, freight_value#14348, product_quantity#14343, quantity#14344]\n                                       +- Project [order_id#14346, order_item_id#14342, cast(product_id#14345 as string) AS product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14347, freight_value#14348, product_quantity#14343, quantity#14344]\n                                          +- Project [order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14347, try_cast(freight_value#14341 as double) AS freight_value#14348, product_quantity#14343, quantity#14344]\n                                             +- Project [order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, try_cast(price#14340 as double) AS price#14347, freight_value#14341, product_quantity#14343, quantity#14344]\n                                                +- Project [cast(order_id#14339 as string) AS order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, quantity#14344]\n                                                   +- Project [order_id#14339, order_item_id#14342, cast(product_id#14338 as string) AS product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, quantity#14344]\n                                                      +- Project [order_id#14339, order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, 1 AS quantity#14344]\n                                                         +- Project [order_id#14339, order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, try_cast(order_item_id#14342 as int) AS product_quantity#14343]\n                                                            +- Project [order_id#14339, cast(order_item_id#13278 as string) AS order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341]\n                                                               +- Project [order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, try_cast(freight_value#14336 as double) AS freight_value#14341]\n                                                                  +- Project [order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, try_cast(price#14335 as double) AS price#14340, freight_value#14336]\n                                                                     +- Project [cast(order_id#14337 as string) AS order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n                                                                        +- Project [order_id#14337, order_item_id#13278, cast(product_id#13279 as string) AS product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n                                                                           +- Project [cast(order_id#13277 as string) AS order_id#14337, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n                                                                              +- Project [order_id#13277, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, price#14335, cast(freight_value#13283 as double) AS freight_value#14336]\n                                                                                 +- Project [order_id#13277, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, cast(price#13282 as double) AS price#14335, freight_value#13283]\n                                                                                    +- Relation [order_id#13277,order_item_id#13278,product_id#13279,seller_id#13280,shipping_limit_date#13281,price#13282,freight_value#13283] csv\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:552)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:401)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:133)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:144)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:107)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:69)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:69)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:54)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:260)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:53)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:114)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:382)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:345)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:367)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:345)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:563)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:343)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:375)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:382)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`order_id`, `review_id`, `review_score`, `review_comment_title`, `review_creation_date`]. SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "42703",
        "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:552)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:401)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:133)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:144)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:107)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:69)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:69)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:54)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:260)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:53)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:114)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:382)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:345)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:367)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:345)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:563)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:343)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:375)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:382)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5926941429959711>, line 21\u001B[0m\n\u001B[1;32m     14\u001B[0m df_reviews_with_product \u001B[38;5;241m=\u001B[39m df_order_reviews\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m     15\u001B[0m     df_order_items\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdistinct(),\n\u001B[1;32m     16\u001B[0m     on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     18\u001B[0m )\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Check schema and columns after join - just to confirm\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m df_reviews_with_product\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m     22\u001B[0m df_reviews_with_product\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# 4. Calculate average review score per product\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2027\u001B[0m, in \u001B[0;36mDataFrame.printSchema\u001B[0;34m(self, level)\u001B[0m\n\u001B[1;32m   2025\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mtreeString(level))\n\u001B[1;32m   2026\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2027\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mtreeString())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2000\u001B[0m, in \u001B[0;36mDataFrame.schema\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1998\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1999\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2000\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mschema(query)\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1268\u001B[0m, in \u001B[0;36mSparkConnectClient.schema\u001B[0;34m(self, plan)\u001B[0m\n\u001B[1;32m   1266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m logger\u001B[38;5;241m.\u001B[39misEnabledFor(logging\u001B[38;5;241m.\u001B[39mINFO):\n\u001B[1;32m   1267\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSchema for plan: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_proto_to_string(plan,\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1268\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mplan)\u001B[38;5;241m.\u001B[39mschema\n\u001B[1;32m   1269\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1270\u001B[0m \u001B[38;5;66;03m# Server side should populate the struct field which is the schema.\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1546\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1545\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1546\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2133\u001B[0m                 info,\n\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`order_id`, `review_id`, `review_score`, `review_comment_title`, `review_creation_date`]. SQLSTATE: 42703;\n'Join UsingJoin(LeftOuter, [order_id])\n:- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:  +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:     +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:        +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:           +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:              +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:                 +- 'Project [unresolvedstarwithcolumns(review_score, try_cast('review_score as double), Some(List({})))]\n:                    +- 'Project [unresolvedstarwithcolumns(order_id, cast('order_id as string), Some(List({})))]\n:                       +- 'Project [review_id#13751, order_id#14983, review_score#14988, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757, cast('product_id as string) AS product_id#14989]\n:                          +- Project [review_id#13751, order_id#14983, try_cast(review_score#14987 as double) AS review_score#14988, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                             +- Project [review_id#13751, order_id#14983, try_cast(review_score#14986 as double) AS review_score#14987, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                +- Project [review_id#13751, order_id#14983, cast(review_score#14985 as double) AS review_score#14986, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                   +- Project [review_id#13751, order_id#14983, try_cast(review_score#14984 as double) AS review_score#14985, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                      +- Project [review_id#13751, order_id#14983, cast(review_score#14982 as double) AS review_score#14984, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                         +- Project [review_id#13751, cast(order_id#13752 as string) AS order_id#14983, review_score#14982, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                            +- Project [review_id#13751, order_id#13752, cast(review_score#13753 as double) AS review_score#14982, review_comment_title#13754, review_comment_message#13755, review_creation_date#13756, review_answer_timestamp#13757]\n:                                               +- Relation [review_id#13751,order_id#13752,review_score#13753,review_comment_title#13754,review_comment_message#13755,review_creation_date#13756,review_answer_timestamp#13757] csv\n+- Deduplicate [order_id#14358, product_id#14357]\n   +- Project [order_id#14358, product_id#14357]\n      +- Project [order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14359, try_cast(freight_value#14356 as double) AS freight_value#14360, product_quantity#14343, quantity#14344]\n         +- Project [order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, try_cast(price#14355 as double) AS price#14359, freight_value#14356, product_quantity#14343, quantity#14344]\n            +- Project [cast(order_id#14354 as string) AS order_id#14358, order_item_id#14342, product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14355, freight_value#14356, product_quantity#14343, quantity#14344]\n               +- Project [order_id#14354, order_item_id#14342, cast(product_id#14353 as string) AS product_id#14357, seller_id#13280, shipping_limit_date#13281, price#14355, freight_value#14356, product_quantity#14343, quantity#14344]\n                  +- Project [order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14355, try_cast(freight_value#14352 as double) AS freight_value#14356, product_quantity#14343, quantity#14344]\n                     +- Project [order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, try_cast(price#14351 as double) AS price#14355, freight_value#14352, product_quantity#14343, quantity#14344]\n                        +- Project [cast(order_id#14350 as string) AS order_id#14354, order_item_id#14342, product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14351, freight_value#14352, product_quantity#14343, quantity#14344]\n                           +- Project [order_id#14350, order_item_id#14342, cast(product_id#14349 as string) AS product_id#14353, seller_id#13280, shipping_limit_date#13281, price#14351, freight_value#14352, product_quantity#14343, quantity#14344]\n                              +- Project [order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14351, try_cast(freight_value#14348 as double) AS freight_value#14352, product_quantity#14343, quantity#14344]\n                                 +- Project [order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, try_cast(price#14347 as double) AS price#14351, freight_value#14348, product_quantity#14343, quantity#14344]\n                                    +- Project [cast(order_id#14346 as string) AS order_id#14350, order_item_id#14342, product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14347, freight_value#14348, product_quantity#14343, quantity#14344]\n                                       +- Project [order_id#14346, order_item_id#14342, cast(product_id#14345 as string) AS product_id#14349, seller_id#13280, shipping_limit_date#13281, price#14347, freight_value#14348, product_quantity#14343, quantity#14344]\n                                          +- Project [order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14347, try_cast(freight_value#14341 as double) AS freight_value#14348, product_quantity#14343, quantity#14344]\n                                             +- Project [order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, try_cast(price#14340 as double) AS price#14347, freight_value#14341, product_quantity#14343, quantity#14344]\n                                                +- Project [cast(order_id#14339 as string) AS order_id#14346, order_item_id#14342, product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, quantity#14344]\n                                                   +- Project [order_id#14339, order_item_id#14342, cast(product_id#14338 as string) AS product_id#14345, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, quantity#14344]\n                                                      +- Project [order_id#14339, order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, product_quantity#14343, 1 AS quantity#14344]\n                                                         +- Project [order_id#14339, order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341, try_cast(order_item_id#14342 as int) AS product_quantity#14343]\n                                                            +- Project [order_id#14339, cast(order_item_id#13278 as string) AS order_item_id#14342, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, freight_value#14341]\n                                                               +- Project [order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14340, try_cast(freight_value#14336 as double) AS freight_value#14341]\n                                                                  +- Project [order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, try_cast(price#14335 as double) AS price#14340, freight_value#14336]\n                                                                     +- Project [cast(order_id#14337 as string) AS order_id#14339, order_item_id#13278, product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n                                                                        +- Project [order_id#14337, order_item_id#13278, cast(product_id#13279 as string) AS product_id#14338, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n                                                                           +- Project [cast(order_id#13277 as string) AS order_id#14337, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, price#14335, freight_value#14336]\n                                                                              +- Project [order_id#13277, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, price#14335, cast(freight_value#13283 as double) AS freight_value#14336]\n                                                                                 +- Project [order_id#13277, order_item_id#13278, product_id#13279, seller_id#13280, shipping_limit_date#13281, cast(price#13282 as double) AS price#14335, freight_value#13283]\n                                                                                    +- Relation [order_id#13277,order_item_id#13278,product_id#13279,seller_id#13280,shipping_limit_date#13281,price#13282,freight_value#13283] csv\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:552)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:489)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:401)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:133)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:144)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:107)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:69)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:69)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:54)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:260)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:53)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:114)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:382)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:345)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:367)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:345)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:563)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:343)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:375)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:382)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, avg, countDistinct, expr\n",
    "\n",
    "# 1. Cast numeric columns properly in order_items\n",
    "df_order_items = df_order_items.withColumn(\"product_id\", col(\"product_id\").cast(\"string\")) \\\n",
    "                               .withColumn(\"order_id\", col(\"order_id\").cast(\"string\")) \\\n",
    "                               .withColumn(\"price\", expr(\"try_cast(price as double)\")) \\\n",
    "                               .withColumn(\"freight_value\", expr(\"try_cast(freight_value as double)\"))\n",
    "\n",
    "# 2. Cast numeric columns properly in order_reviews\n",
    "df_order_reviews = df_order_reviews.withColumn(\"order_id\", col(\"order_id\").cast(\"string\")) \\\n",
    "                                   .withColumn(\"review_score\", expr(\"try_cast(review_score as double)\"))\n",
    "\n",
    "# 3. Join order_reviews with order_items on order_id to get product_id for each review\n",
    "df_reviews_with_product = df_order_reviews.join(\n",
    "    df_order_items.select(\"order_id\", \"product_id\").distinct(),\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check schema and columns after join - just to confirm\n",
    "df_reviews_with_product.printSchema()\n",
    "df_reviews_with_product.show(5)\n",
    "\n",
    "# 4. Calculate average review score per product\n",
    "product_reviews = df_reviews_with_product.groupBy(\"product_id\").agg(\n",
    "    avg(\"review_score\").alias(\"average_review_score\"),\n",
    "    countDistinct(\"order_id\").alias(\"review_count\")\n",
    ")\n",
    "\n",
    "# 5. Calculate product sales metrics\n",
    "product_sales = df_order_items.groupBy(\"product_id\").agg(\n",
    "    sum(expr(\"1\")).alias(\"total_quantity_sold\"),  # Assuming each row = 1 quantity\n",
    "    sum(col(\"price\") + col(\"freight_value\")).alias(\"total_revenue\"),\n",
    "    countDistinct(\"order_id\").alias(\"unique_orders\")\n",
    ")\n",
    "\n",
    "# 6. Join sales and reviews to get overall product popularity\n",
    "product_popularity = product_sales.join(product_reviews, \"product_id\", \"left\")\n",
    "\n",
    "display(product_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0648606f-1d18-4fd6-b391-387800718aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- order_id: string (nullable = true)\n |-- order_item_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- shipping_limit_date: string (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n |-- product_quantity: integer (nullable = true)\n |-- quantity: integer (nullable = false)\n\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----------------+--------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|product_quantity|quantity|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----------------+--------+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|               1|       1|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|               1|       1|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|               1|       1|\n|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|               1|       1|\n|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|               1|       1|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+----------------+--------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_order_items.printSchema()\n",
    "df_order_items.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ab4d2d-478f-4ef9-8132-4f1db1f5d0cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------------+\n|order_month|monthly_revenue   |monthly_order_count|\n+-----------+------------------+-------------------+\n|2016-09    |354.75            |3                  |\n|2016-10    |56808.84          |308                |\n|2016-12    |19.62             |1                  |\n|2017-01    |137188.49000000005|789                |\n|2017-02    |286280.6199999999 |1733               |\n|2017-03    |432048.58999999973|2641               |\n|2017-04    |412422.23999999976|2391               |\n|2017-05    |586190.9499999995 |3660               |\n|2017-06    |502963.03999999975|3217               |\n|2017-07    |584971.6200000002 |3969               |\n|2017-08    |668204.5999999999 |4293               |\n|2017-09    |720398.9100000006 |4243               |\n|2017-10    |769312.3700000001 |4568               |\n|2017-11    |1179143.7700000012|7451               |\n|2017-12    |863547.23         |5624               |\n|2018-01    |1107301.8899999992|7220               |\n|2018-02    |986908.9600000008 |6694               |\n|2018-03    |1155126.8200000017|7188               |\n|2018-04    |1159698.0400000003|6934               |\n|2018-05    |1149781.8200000003|6853               |\n+-----------+------------------+-------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    " # montly revenue and order count trend\n",
    "from pyspark.sql.functions import col, sum, countDistinct, to_date, date_format, expr\n",
    "\n",
    "# Cast needed columns\n",
    "df_orders = df_orders.withColumn(\"order_id\", col(\"order_id\").cast(\"string\")) \\\n",
    "                     .withColumn(\"order_purchase_timestamp\", col(\"order_purchase_timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "df_order_items = df_order_items.withColumn(\"order_id\", col(\"order_id\").cast(\"string\")) \\\n",
    "                               .withColumn(\"price\", expr(\"try_cast(price as double)\")) \\\n",
    "                               .withColumn(\"freight_value\", expr(\"try_cast(freight_value as double)\"))\n",
    "\n",
    "# Join orders and order items on order_id\n",
    "df_order_joined = df_orders.join(df_order_items, on=\"order_id\", how=\"inner\")\n",
    "\n",
    "# Extract month and year from order_purchase_timestamp as YYYY-MM\n",
    "df_order_joined = df_order_joined.withColumn(\"order_month\", date_format(col(\"order_purchase_timestamp\"), \"yyyy-MM\"))\n",
    "\n",
    "# Aggregate monthly revenue and order count\n",
    "monthly_trend = df_order_joined.groupBy(\"order_month\").agg(\n",
    "    sum(col(\"price\") + col(\"freight_value\")).alias(\"monthly_revenue\"),\n",
    "    countDistinct(\"order_id\").alias(\"monthly_order_count\")\n",
    ").orderBy(\"order_month\")\n",
    "\n",
    "monthly_trend.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4221143a-f1d0-44f6-ba4f-33454b3b6ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+-------------------+--------------+\n|customer_id                     |first_order_date   |last_order_date    |retention_days|\n+--------------------------------+-------------------+-------------------+--------------+\n|f7398fc942c8fa80e5419ae52e49f7fb|2018-04-15 19:42:06|2018-04-15 19:42:06|0             |\n|d0b0b2dd8bdaf36ebea19c232b4e986b|2018-06-08 16:47:20|2018-06-08 16:47:20|0             |\n|66e8039d5fddd75067de6b08e4bb22e7|2017-12-04 17:01:15|2017-12-04 17:01:15|0             |\n|c2928a50aecf1bc4776082b13225e4da|2018-01-15 18:00:35|2018-01-15 18:00:35|0             |\n|ba76714c4894372325ea2d044823344a|2017-05-15 08:58:47|2017-05-15 08:58:47|0             |\n|04fc2ecbb192c71163629f423d57a13d|2017-11-02 17:35:55|2017-11-02 17:35:55|0             |\n|f3457b8fdac18622de498551383ae1cc|2017-09-13 18:49:50|2017-09-13 18:49:50|0             |\n|0bd683b7ceca26b5bba4e327682275c5|2017-10-28 20:16:56|2017-10-28 20:16:56|0             |\n|b1e99a86b163f1f25e7e0fa3360ad93d|2018-08-22 01:35:54|2018-08-22 01:35:54|0             |\n|69009aaf4e400602c05b1da3989ca40b|2017-09-01 10:42:58|2017-09-01 10:42:58|0             |\n|4c9b48364e100b48b70b65f7223518bb|2017-09-13 23:13:16|2017-09-13 23:13:16|0             |\n|58359e541013053d51a9831dc013005e|2018-06-06 22:58:31|2018-06-06 22:58:31|0             |\n|5201af6599e76f445ebcb42b7e7dc469|2017-10-15 11:34:39|2017-10-15 11:34:39|0             |\n|929f29371db3b343d53e200d3f2d2fd6|2018-06-20 22:36:01|2018-06-20 22:36:01|0             |\n|b1579afc7974300db655f47a85f69135|2017-10-13 19:14:08|2017-10-13 19:14:08|0             |\n|8cbe6c3bbaaae2aa9f5d97827a7968d0|2018-01-25 16:21:03|2018-01-25 16:21:03|0             |\n|ce5e2dbebc82fdeb89bb35dfaddd56e2|2017-07-21 09:22:49|2017-07-21 09:22:49|0             |\n|a50a163029fb5f7935f311cfd2f5c6f2|2018-02-15 12:10:37|2018-02-15 12:10:37|0             |\n|7fdc2eaa95af7ee23782559451286547|2017-05-21 11:56:58|2017-05-21 11:56:58|0             |\n|8df60f4b5c7991242bdbde7c034f2057|2018-01-25 00:27:59|2018-01-25 00:27:59|0             |\n+--------------------------------+-------------------+-------------------+--------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# customer retention Analysis (First and Lat Order)\n",
    "from pyspark.sql.functions import col, min, max, datediff\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Cast needed columns\n",
    "df_orders = df_orders.withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\")) \\\n",
    "                     .withColumn(\"order_purchase_timestamp\", col(\"order_purchase_timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Aggregate to find first and last order date per customer\n",
    "customer_retention = df_orders.groupBy(\"customer_id\").agg(\n",
    "    min(\"order_purchase_timestamp\").alias(\"first_order_date\"),\n",
    "    max(\"order_purchase_timestamp\").alias(\"last_order_date\")\n",
    ")\n",
    "\n",
    "# Calculate retention period in days (last - first)\n",
    "customer_retention = customer_retention.withColumn(\n",
    "    \"retention_days\",\n",
    "    datediff(col(\"last_order_date\"), col(\"first_order_date\"))\n",
    ")\n",
    "\n",
    "# Show results\n",
    "customer_retention.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1638b88-8b0e-4d68-90a2-8b89c53321d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+---------+--------+\n|            order_id|Approved|Shipped|Delivered|Canceled|\n+--------------------+--------+-------+---------+--------+\n|e481f51cbdc54678b...|       1|      1|        1|       0|\n|53cdb2fc8bc7dce0b...|       1|      1|        1|       0|\n|47770eb9100c2d0c4...|       1|      1|        1|       0|\n|949d5b44dbf5de918...|       1|      1|        1|       0|\n|ad21c59c0840e6cb8...|       1|      1|        1|       0|\n|a4591c265e18cb1dc...|       1|      1|        1|       0|\n|136cce7faa42fdb2c...|       1|      0|        0|       0|\n|6514b8ad8028c9f2c...|       1|      1|        1|       0|\n|76c6e866289321a7c...|       1|      1|        1|       0|\n|e69bfb5eb88e0ed6a...|       1|      1|        1|       0|\n+--------------------+--------+-------+---------+--------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Order ststus Flags\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "# Example with df_orders, assuming these columns exist and are timestamps or strings\n",
    "\n",
    "df_orders_flagged = df_orders \\\n",
    "    .withColumn(\"Approved\", when(col(\"order_approved_at\").isNotNull(), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"Shipped\", when(col(\"order_delivered_carrier_date\").isNotNull(), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"Delivered\", when(col(\"order_delivered_customer_date\").isNotNull(), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"Canceled\", when(col(\"order_status\") == \"canceled\", lit(1)).otherwise(lit(0)))\n",
    "\n",
    "df_orders_flagged.select(\"order_id\", \"Approved\", \"Shipped\", \"Delivered\", \"Canceled\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddcca81a-3805-4b1b-87fe-7152b6242826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|order_hour|orders_count|\n+----------+------------+\n|         0|        2394|\n|         1|        1170|\n|         2|         510|\n|         3|         272|\n|         4|         206|\n|         5|         188|\n|         6|         502|\n|         7|        1231|\n|         8|        2967|\n|         9|        4785|\n|        10|        6177|\n|        11|        6578|\n|        12|        5995|\n|        13|        6518|\n|        14|        6569|\n|        15|        6454|\n|        16|        6675|\n|        17|        6150|\n|        18|        5769|\n|        19|        5982|\n+----------+------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#Hourly order distribution\n",
    "from pyspark.sql.functions import hour, col, count\n",
    "\n",
    "# Cast timestamp column if needed\n",
    "df_orders = df_orders.withColumn(\"order_purchase_timestamp\", col(\"order_purchase_timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Extract hour from order_purchase_timestamp\n",
    "df_orders_with_hour = df_orders.withColumn(\"order_hour\", hour(col(\"order_purchase_timestamp\")))\n",
    "\n",
    "# Group by hour and count orders\n",
    "hourly_order_dist = df_orders_with_hour.groupBy(\"order_hour\").agg(\n",
    "    count(\"order_id\").alias(\"orders_count\")\n",
    ").orderBy(\"order_hour\")\n",
    "\n",
    "hourly_order_dist.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc86d63-3845-4a4f-9d32-260b466766ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n|order_day_type|orders_count|\n+--------------+------------+\n|       Weekday|       76594|\n|       Weekend|       22847|\n+--------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Weekday VS Weekend Orders\n",
    "from pyspark.sql.functions import dayofweek, when, col, count\n",
    "\n",
    "# Cast order_purchase_timestamp as timestamp if not already\n",
    "df_orders = df_orders.withColumn(\"order_purchase_timestamp\", col(\"order_purchase_timestamp\").cast(\"timestamp\"))\n",
    "# Extract day of week (1=Sunday, 2=Monday, ..., 7=Saturday)\n",
    "df_orders = df_orders.withColumn(\"day_of_week\", dayofweek(col(\"order_purchase_timestamp\")))\n",
    "# Create a new column 'order_day_type' to flag Weekday vs Weekend\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"order_day_type\",\n",
    "    when(col(\"day_of_week\").isin([1,7]), \"Weekend\").otherwise(\"Weekday\")\n",
    ")\n",
    "# Group by day type and count orders\n",
    "weekday_vs_weekend = df_orders.groupBy(\"order_day_type\").agg(\n",
    "    count(\"order_id\").alias(\"orders_count\")\n",
    ")\n",
    "weekday_vs_weekend.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Integration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}